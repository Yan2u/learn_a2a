### image_description

A simple project to test multimodal data (text, image, audio) with google a2a protocol.

Use 3 agents to:

- Use pure text prompts and image to generate descriptions.
- Use audio prompts and image to generate descriptions.

#### Agents

- Image Descriptor Agent: Generate descriptions using `gpt-4o` with given image and text prompts.
- Speech2Text Agent: Convert audio to text using `gpt-4o-audio`.
- User Agent: Call agents above to deal with user inputs. Both `text+image` and `audio+image` forms are accepted.

#### Files

- `test.jpg`: The image to test.
- `test.wav`: The audio form of `PROMPT` in `test_client.py` (generated by ai).

#### Usage

1. Initialize venv and install packages using `uv` (in root folder `learn_a2a`).

```bash
# in learn_a2a
uv venv
source .venv/bin/activate
uv sync
```

2. Create/Edit `.env` file to configure the project (in `learn_a2a/image_description`):

```bash
# learn_a2a/image_description/.env
API_KEY=${openai_api_key}
BASE_URL=https://api.openai.com/v1
IMAGE_DESCRIPTOR_PORT=8003 # image descriptor agent port
SPEECH2TEXT_PORT=8004 # speech2text agent port
USER_PORT=8005 # user agent port
```

3. Launch 3 agents (in 3 different terminals)

```bash
# terminal 1
# in learn_a2a/image_description
uv run image_descriptor.py

# terminal 2
# in learn_a2a/image_description
uv run speech2text.py

# terminal 3
# in learn_a2a/image_description
uv run user.py
```

4. Run tests

```bash
# in learn_a2a/image_description
uv run test_client.py
```

